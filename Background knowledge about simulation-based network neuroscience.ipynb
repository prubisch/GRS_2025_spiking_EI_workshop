{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408fe7e3",
   "metadata": {},
   "source": [
    "# Basics of simulation based computational neuroscience\n",
    "\n",
    "\n",
    "This notebook aims to explain the principles and techniques of simulation-based network neuroscience. Simulation-based insights are particularly helpful in a system where you cannot derive insights analytically due to the differential equations being non-solvable or including non-linearities. Such a non-linearity is e.g. the reset of the membrane potential in simple spiking neuron models. \n",
    "\n",
    "In neuroscience we want to model neural behaviour and have a plethora of neuron models to chose from to do so. They vary in complexity and biological plausibility. In general it is true that the higher the complexity of the model the more computational resources are needed for simulating the system. Therefore, biological realism comes at the cost of computational effeciency. \n",
    "\n",
    "The computationally most effecient models are rate-based neuron models. An added bonus of them is, that they can be analysed analytically. The rate-based workshop is giving you some more information and insight into this. \n",
    "\n",
    "For neuron models which are defined by non-solvable differential equations like the simple Leaky-Integrate and Fire (LIF) neuron or the Hodgkin-Huxley neuron model, we need to use numerical integration to solve the equation. There are different numerical integration methods and in the following we will discuss the Euler method which is also used by simulation software such as BRIAN.\n",
    "\n",
    "\n",
    "# Numerical Integration: The Euler method\n",
    "\n",
    "Models in computational neuroscience are predominantely based on **(Ordinary) Differential Equations** (ODEs). Differential equations define the change of a system's state with regard to a certain quantity. In neuroscience, we often want to understand how the state of a neuron changes over time given an external stimulus or neuromodulation. Therefore neural ODEs or neural models define the change of neural activity $x$ (e.g. the rate or the membrane potential) over time: \n",
    "\\begin{align*}\\frac{dx}{dt} = f(x(t))\\end{align*}. \n",
    "Here $x(t)$ corresponds to the neural state at time $t$ and $f(x(t))$ defines how the neural activity changes over the integration time step $t$. \n",
    "\n",
    "Given the differential equations of this form, we can simulate how the neural activity develops given an initial value $x(t=0)$ (also called the initial condition). We do this by calculating the successive neural states starting with our intial condition: \n",
    "\\begin{align*} x(t=1) &= x(t=0) + f(x(t=0)) \\\\ x(t=2) &= x(t=1) + f(x(t=1)) \\\\ &... \\\\ x(t=T) &= x(t=T-1) + f(x(t=T-1))\\end{align*}\n",
    "with $T$ being the last timestep we want to simulate. Formally the Euler method can be thus defined as \n",
    "\\begin{align*} x(t+dt) = x(t) + \\frac{dx}{dt} \\end{align*}. \n",
    "$dt$ is the integration time step. In other words the temporal resolution we use to simulate the system. The smaller the integration time step $dt$, the smaller the error between the \"true value\" of $x(t+dt)$ and the approximation $x(t) + \\frac{dx}{dt}$.\n",
    "\n",
    "The graph below shows the Euler method graphically. In this case we know the true form of $x(t)$ at each point. Therefore, we can that if $dt$ is big the discrepancy between the true value and the approximation via the Euler method is big as well. Therefore, $dt$ should always be small. For neuron models $dt$= 0.1ms is a standard value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8704ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patricia/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e4a5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14398f9344b645779011093f5229aecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, description='dt:', max=0.2, min=0.001, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b737d8c16dcc418a86784e5888486ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def euler_method(dt,init, start, end, fun):\n",
    "\n",
    "    steps = int((end-start)/dt)\n",
    "    vals = np.arange(start, end+dt, dt)\n",
    "    vals[0] = init\n",
    "    for ind, v in enumerate(vals[:-1]):\n",
    "        vals[ind+1] = vals[ind] + dt * fun(vals[ind])\n",
    "\n",
    "    return vals\n",
    "\n",
    "def ODE(x):\n",
    "    return 2 * x\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "init_x = 2\n",
    "\n",
    "# Function that generates and plots data based on the slider value\n",
    "def update_plot(value):\n",
    "\n",
    "    t = np.arange(start, end+value, value)# Example function using the slider value\n",
    "    x_approx = euler_method(value, init_x, start, end, ODE)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(t, init_x * np.exp(2*t),color = 'grey')\n",
    "    plt.plot(t, x_approx, color = 'black', label = 'approximation')\n",
    "    plt.plot([0,value],[init_x,x_approx[0]], color = 'red', label = 'dt')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "    plt.xlim(start, end)\n",
    "    plt.ylim(1.5, 15)\n",
    "    plt.title('Euler integration step ='+str(value))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Create a slider\n",
    "slider = widgets.FloatSlider(value=0.1, min=0.01, max=0.2, step=0.01, description=\"dt:\")\n",
    "\n",
    "# Use interactive to update the plot when the slider moves\n",
    "interactive_plot = widgets.interactive_output(update_plot, {'value': slider})\n",
    "\n",
    "# Display slider and interactive plot\n",
    "display(slider, interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5852e17",
   "metadata": {},
   "source": [
    "# Neuronal differential equations\n",
    "\n",
    "In this workshop you will encounter two different types of neuron models. The rate model and the spiking neuron model. As mentioned before rate models can be treated analytically, spiking models can not. But to see the difference in the formulation, we will demonstrate how to simulate them both using the Euler method. \n",
    "\n",
    "The rate model describes neuronal activation using a single ODE which represents the firing rate. A typical rate neuron model is the following:\n",
    "\n",
    "$\\tau \\frac{dr}{dt} = -r + I(t)$\n",
    "\n",
    "where $I(t)$ is an external stimulus which can be turned on and off.\n",
    "\n",
    "we can unpack the equation a bit more\n",
    "\n",
    "Below you can find an example. Try and play around with the stimulus strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_neuron(r, I,tau = 10):\n",
    "    return (-r + I)/tau\n",
    "\n",
    "def integrate(neuron, time, stimulus, dt = 0.1):\n",
    "    sim_time = int(time/dt)\n",
    "    r_act = np.zeros([sim_time+1]) #init activation is 0 here\n",
    "    t_record = np.zeros([sim_time])\n",
    "    I = 0\n",
    "    I_record =np.zeros([sim_time])\n",
    "\n",
    "    for t in range(sim_time):\n",
    "        if t > int(10/dt) and t < int(stimulus[1]/dt):\n",
    "            I = stimulus[0]\n",
    "        else:\n",
    "            I = 0\n",
    "        r_act[t+1] = r_act[t]+ dt * (rate_neuron(r_act[t],I))\n",
    "        t_record[t] = dt * t\n",
    "        I_record[t] = I\n",
    "\n",
    "    return r_act[:-1], t_record, I_record\n",
    "\n",
    "\n",
    "def update_rateplot(I_strength,I_duration ):\n",
    "\n",
    "    rates, ts, stim = integrate(rate_neuron, 100, [I_strength,I_duration])#our time is in ms here\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(ts, rates,label = 'r')\n",
    "    plt.plot(ts, stim, color = 'grey', label = 'I')\n",
    "    plt.xlabel('t[ms]')\n",
    "    plt.ylabel('r')\n",
    "    plt.title('Rate neuron')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Create a slider\n",
    "slider = widgets.FloatSlider(value=10, min=0, max=100, step=1, description=\"strength:\")\n",
    "slider_l = widgets.FloatSlider(value=50, min=1, max=90, step=1, description=\"length[ms]:\")\n",
    "\n",
    "# Use interactive to update the plot when the slider moves\n",
    "interactive_plot = widgets.interactive_output(update_rateplot, {'I_strength': slider,'I_duration': slider_l})\n",
    "\n",
    "# Display slider and interactive plot\n",
    "display(slider, slider_l, interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf046336",
   "metadata": {},
   "source": [
    "A spiking neuron model is chracterised by the addition of a spike generation mechanism. In a Leaky-Integrate and Fire (LIF) model this is implemented using a spike threshold and reset: If the membrane potential $v$ surpasses the defined spike threshold $\\Theta_s$, the current membrane potential is set to the reset potential, e.g. to the resting potential $E_l$. This reset is a non-linearity as the membrane-potential after the reset does not linearly depend on its value prior to the reset. \n",
    "\n",
    "Mathematically the LIF neuron is defined as:\n",
    "\n",
    "\\begin{align*} \\tau \\frac{dv}{dt} = (E_l - v) + I(t) \\\\\n",
    "if \\ \\ v(t) > \\Theta_s: v(t) = E_l \\end{align*}\n",
    "\n",
    "where $v$ is the neuron's membrane potential, $E_l$ is the resting potential, $\\tau$ is the membrane potential time constant, and $\\Theta_s$ is the spiking threshold.\n",
    "\n",
    "More complex version exists which includes differential equations for After-Spike reset currents, spike-threshold adaption, etc..\n",
    "\n",
    "Below you can vary the received input to visualise its effect on the spiking rate and below threshold behaviour. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967959d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIF_neuron(v, I,Theta_s = -50, E_L = -70, tau = 10):\n",
    "    if v < Theta_s:\n",
    "        return ((E_L - v)+ I)/tau\n",
    "    else:\n",
    "        return E_L # immediate reset, we do not approximate the spike form here\n",
    "\n",
    "def integrate(neuron, time, stimulus, dt = 0.1):\n",
    "    sim_time = int(time/dt)\n",
    "    v_act = np.zeros([sim_time+1])-70 # init activation is E_L here\n",
    "    t_record = np.zeros([sim_time])\n",
    "    I = 0\n",
    "    I_record = np.zeros([sim_time])\n",
    "\n",
    "    for t in range(sim_time):\n",
    "        if t > int(10/dt) and t < int(stimulus[1]/dt): # define stimulus\n",
    "            I = stimulus[0]\n",
    "        else:\n",
    "            I = 0\n",
    "        v_act[t+1] = v_act[t]+ dt * (neuron(v_act[t], I)) # calculate potential for next time step\n",
    "        t_record[t] = dt * t\n",
    "        I_record[t] = I\n",
    "\n",
    "    return v_act[:-1], t_record, I_record, np.nonzeros(vs >= -50)[0]\n",
    "\n",
    "\n",
    "def update_rateplot(I_strength,I_duration ):\n",
    "\n",
    "    vs, ts, stim,spikes = integrate(LIF_neuron, 100, [I_strength,I_duration])#our time is in ms here\n",
    " \n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(ts, vs,label = 'v')\n",
    "    plt.scatter(spikes, spikes * 0 + -45, color = 'r')\n",
    "    plt.plot(ts, stim, color = 'grey', label = 'I')\n",
    "    plt.xlabel('t[ms]')\n",
    "    plt.ylabel('v')\n",
    "    plt.title('LIF neuron')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Create a slider\n",
    "slider = widgets.FloatSlider(value=10, min=0, max=100, step=1, description=\"strength:\")\n",
    "slider_l = widgets.FloatSlider(value=50, min=1, max=90, step=1, description=\"length[ms]:\")\n",
    "\n",
    "# Use interactive to update the plot when the slider moves\n",
    "interactive_plot = widgets.interactive_output(update_rateplot, {'I_strength': slider,'I_duration': slider_l})\n",
    "\n",
    "# Display slider and interactive plot\n",
    "display(slider, slider_l, interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fd311",
   "metadata": {},
   "source": [
    "# Noise\n",
    "\n",
    "To drive neural activity one can either insert a constant external stimuli or noise into the neuron. Simulating noise has the advantage that it approximates the synaptic input from a large amount of neurons one does not need to model explicitly. \n",
    "\n",
    "The noisy current $\\eta$ is commonly defined as a Ornstein-Uhlenbeck (OU) process. OU noise looks like a random walk. It integrates a Wiener process $\\frac{dW}{dt}$ (time step adjusted white noise) overtime. It is defined as: \n",
    "\\begin{align*}\n",
    "\\tau \\frac{d\\eta}{dt} = \\theta \\cdot (\\mu - \\eta) + \\sigma \\underbrace{\\frac{dW}{dt}}_{\\text{Wiener process}} \n",
    "\\end{align*}\n",
    " with $\\mu$ is the mean value around which the process fluctuates, $\\theta$ is the drive towards the mean and $\\sigma$ the spread of the process. The spread defines how strongly the noise fluctuates around its mean. The bigger $\\sigma$ the higher are the deviations from the $\\mu$. The time constant $\\tau$ defines the fluctuation speed. The higher the the time constant, the slower the fluctuations. The drive $\\theta$ determines how fast the OU process approches $\\mu$ after initialisation. \n",
    "\n",
    "In practice we simulate this noise process using the Euler method, which gives the following equation:\n",
    "\\begin{align*}\n",
    "\\eta(t+1) = \\eta(t) + \\frac{dt}{\\tau} \\theta \\cdot (\\mu - \\eta) + \\sigma  \\underbrace{\\sqrt{\\frac{2dt}{\\tau}}\\cdot N(0,1)}_{\\text{Wiener process}}\n",
    "\\end{align*}\n",
    "\n",
    "The scaling of the Wiener process is necessary to ensure independence of the spread $\\sigma$ and the integration time step.\n",
    "\n",
    "Below you can try how the activity of a rate neuron and a LIF (same formulations as above) differ depending on the standard deviation and mean of the OU noise. ($ \\theta = 1$ and therefore omitted in the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9626e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed = 5) #commentar zum random seed, eventuell wegnehmen\n",
    "\n",
    "def integrate(time, stimulus, dt = 0.1, tau = 20):\n",
    "    rng = np.random.RandomState(seed = 5)\n",
    "    sim_time = int(time/dt)\n",
    "    v_act = np.zeros([sim_time+1])-70 #init activation is E_L here\n",
    "    r_act = np.zeros([sim_time+1])\n",
    "    t_record = np.zeros([sim_time])\n",
    "    I =np.zeros([sim_time+1])+stimulus[0] #we start at the mean value so we do not need a warm-up period to get the statistics correct\n",
    "\n",
    "    for t in range(sim_time):\n",
    "        I[t+1] = I[t] + dt /tau*(stimulus[0] - I[t]) + np.sqrt(2*dt/tau) * stimulus[1] * rng.randn()\n",
    "        v_act[t+1] = v_act[t]+ dt * (LIF_neuron(v_act[t],I[t]))\n",
    "        r_act[t+1] = r_act[t]+ dt * (rate_neuron(r_act[t],I[t]))\n",
    "        t_record[t] = dt * t\n",
    "\n",
    "    return v_act[:-1], r_act[:-1], t_record, I[:-1]\n",
    "\n",
    "\n",
    "def update_noiseplot(mean,sd ):\n",
    "\n",
    "    vs, rs, ts, stim = integrate( 100, [mean,sd])#our time is in ms here\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(3,1,figsize=(6, 6))\n",
    "    ax[0].plot(ts, stim)\n",
    "    ax[0].plot(ts, stim*0+mean, color = 'grey', linestyle = ':')\n",
    "    ax[0].set_xlabel('t[ms]')\n",
    "    ax[0].set_ylabel('$\\eta$')\n",
    "    ax[0].set_title('OU Noise')\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(ts,rs)\n",
    "    ax[1].plot(ts, rs*0, color = 'grey', linestyle = ':')\n",
    "    ax[1].set_xlabel('t[ms]')\n",
    "    ax[1].set_ylabel('$r$')\n",
    "    ax[1].set_title('Rate Neuron')\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    ax[2].plot(ts, vs)\n",
    "    ax[2].plot(ts, vs*0-70, color = 'grey', linestyle = ':')\n",
    "    ax[2].set_xlabel('t[ms]')\n",
    "    ax[2].set_ylabel('$v$')\n",
    "    ax[2].set_title('LIF Neuron')\n",
    "    ax[2].grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "# Create a slider\n",
    "slider = widgets.FloatSlider(value=0, min=-20, max=100, step=1, description=\"μ:\")\n",
    "slider_l = widgets.FloatSlider(value=2, min=0.1, max=20, step=.1, description=\"σ:\")\n",
    "\n",
    "# Use interactive to update the plot when the slider moves\n",
    "interactive_plot = widgets.interactive_output(update_noiseplot, {'mean': slider,'sd': slider_l})\n",
    "\n",
    "# Display slider and interactive plot\n",
    "display(slider, slider_l, interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94da55",
   "metadata": {},
   "source": [
    "# Simulation modules & packages\n",
    "\n",
    "So far, we programmed every detail of the system ourselfes. However, this is strictly not neccessary. There are plenty of python packages and modules specifically dedicated to simulating dynamical systems and networks. This means that you do not have to take care about any implementational details. For most simulation softwares, you only need to define your network. The integration method can be specified, but is implemented effeiciently in the back-end. This enables us to simualte larde networks without having to increase computationally effeciency of our code, as the packages are already tuned for this. Using simulation modules can help to mitigate some of the complexity-computational-cost trade-off. This, finding the right package can help you speed up your simulations significantly: **Neuron**, **Nest**, **BRIAN**, **Dendrify** to name a few. \n",
    "\n",
    "\n",
    "In the following, we will show you how to implement the above noise neuron with **BRIAN**. During the Spiking Neural Network tutorial we will continue using that package. It has an intuitive syntax which makes network set-up easy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc17430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "from brian2tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492572b",
   "metadata": {},
   "source": [
    "To define a model in **BRIAN**, you have to specify it as a differential equation of the form: \n",
    "\n",
    "    differential_equation = ''' dx/dt = dif(x) : unit'''  \n",
    "\n",
    "The *unit* is always given in SI units. **BIRAN** checks your differential equation for unit compatibility. This means that you will get an error message if a differential equation specifying the behaviour of the membrane potential of a neuron does not conform with the units of $v/s$ or if you try to add a variable measured in Hertz to a variable measured in Ampere. This is specifically helpful when having to convert between units. \n",
    "\n",
    "However, you do not have to specify units. You can also define dimensionless variables with a unit of $1$. \n",
    "You can add neural attributes by specifying the variable after the differntial equation:\n",
    "\n",
    "    '''variable:unit'''\n",
    "\n",
    "For example you can specify the membrane time constant that way and give each neuron in your network a different time constant by setting it explicitly. Below you fined the definitions for a dimensionless rate neuron, a rate neuron which activity is measured as a rate explicitly and a LIF neuron. Any variable which is dependent on t like *I_rate(t)*, *I[t]* and *I_spike(t)* are external time-dependent input. Their activity is not part of the neuron definition but is set individually. They can be used to insert a square stimuli into the neuron without having to stop the simulation, or to present repeating stimuli patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differential equations\n",
    "#dimensionless rate neuron\n",
    "diff_eq_rate = '''dr/dt = (-r + I_rate(t))/tau : 1\n",
    "               tau : second'''\n",
    "#Frequency rate neuron\n",
    "diff_eq_rate_Hz = '''dr/dt = (-r + I[t])/tau : Hz\n",
    "               tau : second'''\n",
    "#LIF neuron\n",
    "diff_eq_spike = '''dv/dt = ((E_L - v) + I_spike(t))/tau : volt (unless refractory)\n",
    "                   E_L : volt\n",
    "                   v_spike : volt\n",
    "                   tau : second'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_brian_neurons(length, strength):\n",
    "    start_scope() #to keep the networks clean when running the same entities multiple times\n",
    "    \n",
    "    #Define the neuron. Syntax: NeuronGroup(number of neurons, differential equation)\n",
    "    #we explicitly specify to use the euler method\n",
    "    N_rate = NeuronGroup(1, diff_eq_rate, method = 'euler')\n",
    "    N_rate.tau = 10 * ms\n",
    "    N_spike = NeuronGroup(1, diff_eq_spike, threshold = 'v > v_spike', reset = 'v = E_L', refractory = 3 * ms, method = 'euler') #our spiking neuron has a refactory period of 3ms\n",
    "    #the LIf neuron also has a refractory period during which we stop integration\n",
    "    N_spike.E_L = -79 * mV\n",
    "    N_spike.v_spike = -49 * mV\n",
    "    N_spike.v = -79 * mV  #we need define v(0) otherwise it is assumed to be 0\n",
    "    N_spike.tau = 10 * ms\n",
    "\n",
    "    #we know define the time-dependent input\n",
    "    input_arr = np.zeros([200]) #the stimulus length is 200ms\n",
    "    onset = 10 #our stimulus begins after 10ms\n",
    "    input_arr[onset:int(length)+onset] = strength\n",
    "    I_rate = TimedArray(input_arr, dt = 1*ms) #each entry in the TimedArray lasts for 1ms\n",
    "    I_spike = TimedArray(input_arr * mV, dt = 1*ms) #needs to be in mV to be compatible with dimensions of the model (with proper consideration of conductances we could also inject a current here)\n",
    "\n",
    "    #to record activity we need to define monitors\n",
    "    s_v = SpikeMonitor(N_spike)\n",
    "    \n",
    "    #Syntax: StateMonitor(group to record from, variable to record, which neurons to record from, recording frequency)\n",
    "    record_r = StateMonitor(N_rate, ['r'], record = True, dt = 1 * ms) #record = True means we record from the full neurongroup\n",
    "    record_v = StateMonitor(N_spike, ['v'], record = True, dt = 1 * ms)\n",
    "\n",
    "    run(500 * ms, report = 'text') #run the network for 500ms, you can delete the status report by omitting the second argument in the function\n",
    "\n",
    "    return s_v, record_r, record_v\n",
    "\n",
    "\n",
    "\n",
    "def update_brianplot(stim_l, stim_s ):\n",
    "\n",
    "    spikemon, rec_r, rec_v = run_brian_neurons(stim_l, stim_s)\n",
    "\n",
    "    fig, ax = plt.subplots(2,1,figsize=(6, 6))\n",
    "    ax[0].plot(rec_r.t, rec_r.r[0])\n",
    "    ax[0].set_xlabel('t[ms]')\n",
    "    ax[0].set_ylabel('$r$')\n",
    "    ax[0].set_title('Rate Neuron')\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(rec_v.t,rec_v.v[0]/mV)\n",
    "    ax[1].scatter(spikemon.t, spikemon.i - 45, color = 'r') #we set the indicators for the spike a bit above the spike threshold. if you want to plot a spike raster you just use SpikeMonitor.i\n",
    "    ax[1].set_xlabel('t[ms]')\n",
    "    ax[1].set_ylabel('$v[mV]$')\n",
    "    ax[1].set_title('LIF Neuron')\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "# Create a slider\n",
    "slider = widgets.FloatSlider(value=10, min=0, max=100, step=1, description=\"duration:\")\n",
    "slider_l = widgets.FloatSlider(value=50, min=0, max=100, step=.1, description=\"amplitude:\")\n",
    "\n",
    "# Use interactive to update the plot when the slider moves\n",
    "interactive_plot = widgets.interactive_output(update_brianplot, {'stim_l': slider,'stim_s': slider_l})\n",
    "\n",
    "# Display slider and interactive plot\n",
    "display(slider, slider_l, interactive_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
